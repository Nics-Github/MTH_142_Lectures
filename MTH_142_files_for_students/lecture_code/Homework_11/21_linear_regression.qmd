---
title: "Linear regression one predictor"
author: "Schwab"
format: 
  revealjs:
    theme: beige
editor: visual
execute: 
  echo: true
---

## Read before:

[Read chapter 24 before hand.](https://openintro-ims.netlify.app/inf-model-slr#case-study-sandwich-store)

## Libraries

```{r}
library(openintro)
library(tidyverse)
library(broom)
library(statsr)
```

# Linear Regression (again)

## Recall Linear Regression

```{r}
ggplot(data = starbucks)+
  geom_point(aes(x=fat, y= calories))
```

## Add the model to the points.

```{r}
ggplot(data = starbucks, aes(x=fat, y= calories))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)
```

## Model outputs: tidy

```{r}
star_model <- lm(calories ~ fat, data = starbucks)
#tidy() give the regression output
tidy(star_model) 
```

## summary()

```{r}
summary(star_model)
```

# Are residuals normalish?

```{r}
star_model_aug <- augment(star_model)


ggplot(data = star_model_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## What were we doing?

-   Make a model

-   Trying to figure out if the model is reasonable

    -   looking at the correlation coefficient r.

## What will we do today?

-   A hypothesis test to see if the slope is a number other than zero.

-   recall: $y= \beta_0 + \beta_1 x$

-   $\beta_1$ is the slope.

-   If the slope is zero there is no relationship between y and x.

## Conditions: LINE

-   Linearity\* data has to be *linear*

-   Data has to be *independent*

    -   watch out for time series.

-   nearly *normal* residuals

    -   look for random disbursment around the zero line of residual plot.

-   constant or *equal* *variability*

    -   the points in the residual plot should not have a distinct/changing pattern.

## Some examples:

**Independence problems**

```{r}
ggplot(data= arbuthnot, aes(year,boys))+
  geom_point() +
  geom_smooth(method = "lm" , se = FALSE)
```

## Linear Problems

```{r}
#| echo: false
#| warning: false

ggplot(data= cars93, aes(x = mpg_city, y = price ))+
  geom_point()+
  geom_smooth(method="lm", se=FALSE) +
  labs(title = "Price by MPG cars93")
```

## Normal issues

```{r}
#| echo: false
#| warning: false

ggplot(data= starbucks, aes(x=fiber, y= calories))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)+
  labs(title = "Calories by Fiber",
      subtitle  = "Selected Starbucks Items")
```

## Normal issues again

```{r}
#| echo: false
#| warning: false


star_model_fiber <- lm(calories ~ fiber, data= starbucks)
star_model_fiber_aug <- augment(star_model_fiber)

ggplot(data = star_model_fiber_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## Starbucks calories vs fat.

```{r}
starbucks |>
  ggplot(aes(x=fat, y= calories))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)
```

# Residuals

```{r}
#| echo: false
#| warning: false

ggplot(data = star_model_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## What about this?

```{r}
ggplot(data = arbuthnot, aes(x = girls, y = boys))+
  geom_point()+
  labs(title = "Boys vs Girls",
       subtitle = "Born in London in the 1600s")+
  geom_smooth(method = "lm")

```

## Residuals girl vs boy births

```{r}
#| echo: false
#| warning: false
arbuthnot_fitted <- lm(boys ~ girls, data= arbuthnot)
arbuthnot_fitted_aug <- augment(arbuthnot_fitted)

ggplot(data = arbuthnot_fitted_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  xlab("Fitted values") +
  ylab("Residuals")
```

# Let's do a test

## Recall

$$
y=\beta_0+\beta_1 x + e
$$

-   $\beta_0$ = intercept

-   $\beta_1$ = slope

-   x is the **predictor** variable

-   y is the **response** variable

-   e is the error

## Relationship between variable x and y?

If there is no relationship the slope is 0

If there is a relationship the slope is not zero.

We do

$$
H_0: \beta_1 = 0 \\
H_1: \beta_1 \ne 0
$$

(We could do other tests on r - the correlation coefficient or $\beta_0$ )

## The t distribution

The distribution of the slopes of an infinite number of samples would be a student t.

So we are doing a t test with this test statistic:

$T = \frac{\hat{\beta}_1-0} {\text{SE}}$

df= n-2

We'll let R calculate $SE$ and $\hat{\beta_1}$.

## The test

$$
H_0: \beta_1 = 0 \\
H_1: \beta_1 \ne 0
$$

```{r}
results <- lm(boys ~ girls, data= arbuthnot)
summary(results)
```

## Some Vocab

$r$ - correlation coefficient

$r^2$ - coefficient of determination

$r^2$ is the proportion of the variability that can be explained by the explanatory variable

## Homework Time

[Problem 3 in class.](https://openintro-ims.netlify.app/inf-model-slr#sec-chp24-exercises)
